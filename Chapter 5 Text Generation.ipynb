{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be training an LSTM and Bidirectional LSTM to generate the *same text it is being fed*. As in, we will be training them on sequences of characters with the goal of predicting the next character. Then, we will give them those same sequences hoping that, in each case, they predict the correct next character. By combining those characters, we hope to get a coherent body of text that looks like the original. So, a perfect accuracy would yield the same text as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "import random \n",
    "\n",
    "# NN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Bidirectional\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# PDF manipulation\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "# NLP\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pages = 2\n",
    "pdf_file = 'economics_textbook.pdf'\n",
    "misc = '''... '' -- '''.split()\n",
    "sequence_length = 100\n",
    "window_size = 2\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join([word for word in text if ord(word) < 128])\n",
    "\n",
    "def load_data(raw_text=False, pdf_file=pdf_file, max_pages=max_pages, directory='Data/'+pdf_file):\n",
    "    return_string = StringIO()\n",
    "    device = TextConverter(PDFResourceManager(), return_string, codec='utf-8', laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(PDFResourceManager(), device=device)\n",
    "    filepath = open(directory, 'rb')\n",
    "    for page in PDFPage.get_pages(filepath, set(), maxpages=max_pages, caching=True, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    text_data = return_string.getvalue()\n",
    "    filepath.close(), device.close(), return_string.close()\n",
    "    if raw_text == True: return remove_non_ascii(text_data)\n",
    "    else: text_data = ' '.join([word for word in word_tokenize(remove_non_ascii(text_data)) if word not in stop_words])\n",
    "    return text_data\n",
    "\n",
    "def preprocess_data(sequence_length=sequence_length, max_pages=max_pages, pdf_file=pdf_file):\n",
    "    text_data = load_data(max_pages=max_pages, pdf_file=pdf_file)\n",
    "    characters = list(set(text_data.lower())) # These are the characters that show up in the file\n",
    "    character_dict = dict((character, i) for i, character in enumerate(characters)) # Dictionary with keys as characters and \n",
    "    # values as simply that character's value in a sequenctial list. We will use this to replace characters with numbers, so \n",
    "    # values don't matter, only that they're different\n",
    "    int_dictionary = dict((i, character) for i, character in enumerate(characters)) # Same thing as above with keys and values\n",
    "    # switched\n",
    "    num_chars, vocab_size = len(text_data), len(characters)\n",
    "    x, y = [], []\n",
    "\n",
    "    for i in range(0, num_chars - sequence_length, 1):\n",
    "        input_sequence = text_data[i: i+sequence_length] # Sequences of length 'sequence_length' of letters/characters\n",
    "        output_sequence = text_data[i+sequence_length] # The next letter/character to come\n",
    "        x.append([character_dict[character.lower()] for character in input_sequence]) # Lowercasing all the characters and \n",
    "        # replacing them with their value in the character dictionary\n",
    "        y.append(character_dict[output_sequence.lower()]) # Lowercasing the character and replacing it with the value in the \n",
    "        # character dictionary\n",
    "    \n",
    "    for k in range(0, len(x)): # Changing the shape of x\n",
    "        x[i] = [_x for _x in x[i]]    \n",
    "    x = np.reshape(x, (len(x), sequence_length, 1)) # Further changing the shape of x\n",
    "    x = x/float(vocab_size) \n",
    "    y = np_utils.to_categorical(y) # One-hot encoding of y\n",
    "    return x, y, num_chars, vocab_size, int_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape (1604, 100, 1)\n",
      "Example of a training example for x [[0.73684211]\n",
      " [0.15789474]\n",
      " [0.13157895]\n",
      " [0.21052632]\n",
      " [0.68421053]\n",
      " [0.73684211]\n",
      " [0.92105263]\n",
      " [0.89473684]\n",
      " [0.73684211]\n",
      " [0.68421053]\n",
      " [0.47368421]\n",
      " [0.78947368]\n",
      " [0.47368421]\n",
      " [0.42105263]\n",
      " [0.73684211]\n",
      " [0.92105263]\n",
      " [0.78947368]\n",
      " [0.68421053]\n",
      " [0.73684211]\n",
      " [0.15789474]\n",
      " [0.92105263]\n",
      " [0.68421053]\n",
      " [0.21052632]\n",
      " [0.47368421]\n",
      " [0.97368421]\n",
      " [0.23684211]\n",
      " [0.07894737]\n",
      " [0.31578947]\n",
      " [0.68421053]\n",
      " [0.5       ]\n",
      " [0.07894737]\n",
      " [0.76315789]\n",
      " [0.84210526]\n",
      " [0.78947368]\n",
      " [0.47368421]\n",
      " [0.73684211]\n",
      " [0.13157895]\n",
      " [0.07894737]\n",
      " [0.84210526]\n",
      " [0.68421053]\n",
      " [0.05263158]\n",
      " [0.31578947]\n",
      " [0.92105263]\n",
      " [0.47368421]\n",
      " [0.73684211]\n",
      " [0.13157895]\n",
      " [0.        ]\n",
      " [0.92105263]\n",
      " [0.68421053]\n",
      " [0.05263158]\n",
      " [0.07894737]\n",
      " [0.60526316]\n",
      " [0.60526316]\n",
      " [0.07894737]\n",
      " [0.84210526]\n",
      " [0.21052632]\n",
      " [0.68421053]\n",
      " [0.47368421]\n",
      " [0.73684211]\n",
      " [0.73684211]\n",
      " [0.31578947]\n",
      " [0.13157895]\n",
      " [0.39473684]\n",
      " [0.76315789]\n",
      " [0.73684211]\n",
      " [0.13157895]\n",
      " [0.07894737]\n",
      " [0.84210526]\n",
      " [0.52631579]\n",
      " [0.84210526]\n",
      " [0.07894737]\n",
      " [0.84210526]\n",
      " [0.05263158]\n",
      " [0.07894737]\n",
      " [0.60526316]\n",
      " [0.60526316]\n",
      " [0.92105263]\n",
      " [0.31578947]\n",
      " [0.05263158]\n",
      " [0.13157895]\n",
      " [0.47368421]\n",
      " [0.23684211]\n",
      " [0.52631579]\n",
      " [0.21052632]\n",
      " [0.15789474]\n",
      " [0.47368421]\n",
      " [0.31578947]\n",
      " [0.92105263]\n",
      " [0.47368421]\n",
      " [0.23684211]\n",
      " [0.13157895]\n",
      " [0.36842105]\n",
      " [0.92105263]\n",
      " [0.68421053]\n",
      " [0.71052632]\n",
      " [0.63157895]\n",
      " [0.28947368]\n",
      " [0.68421053]\n",
      " [0.23684211]\n",
      " [0.13157895]]\n",
      "y shape <class 'numpy.ndarray'>\n",
      "Example of a training example for y [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x, y, num_chars, vocab_size, int_dictionary = preprocess_data()\n",
    "print('x shape', x.shape) \n",
    "print('Example of a training example for x', x[0]) # So, we have 1604 sequences, \n",
    "# each one containing 100 values, and each value is in brackets\n",
    "print('y shape', type(y))\n",
    "print('Example of a training example for y', y[0]) # One-hot encoding of the value of the next character to come in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def train_rnn_keras(epochs, activation, num_units): \n",
    "    \n",
    "    x, y, num_chars, vocab_size, int_dictionary = preprocess_data()\n",
    "    \n",
    "    def create_rnn(num_units=num_units, activation=activation):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(num_units, activation=activation, input_shape=(None, x.shape[1])))\n",
    "        model.add(Dense(y.shape[1], activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')            \n",
    "        model.summary()\n",
    "        return model\n",
    "            \n",
    "    rnn_model = create_rnn()\n",
    "    _x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "    rnn_model.fit(_x, y, epochs=epochs, shuffle=True)\n",
    "    \n",
    "    return rnn_model, _x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Text Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 300)               481200    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 38)                11438     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 492,638\n",
      "Trainable params: 492,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "51/51 [==============================] - 2s 4ms/step - loss: 3.1696\n",
      "Epoch 2/100\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 3.0054\n",
      "Epoch 3/100\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 2.9760\n",
      "Epoch 4/100\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 2.9469\n",
      "Epoch 5/100\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 2.9222\n",
      "Epoch 6/100\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 2.8828\n",
      "Epoch 7/100\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 2.8440\n",
      "Epoch 8/100\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 2.8013\n",
      "Epoch 9/100\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 2.7693\n",
      "Epoch 10/100\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 2.7345\n",
      "Epoch 11/100\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 2.6918\n",
      "Epoch 12/100\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 2.6567\n",
      "Epoch 13/100\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 2.5978\n",
      "Epoch 14/100\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 2.5800\n",
      "Epoch 15/100\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 2.5474\n",
      "Epoch 16/100\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 2.5076\n",
      "Epoch 17/100\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 2.4873\n",
      "Epoch 18/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.4636\n",
      "Epoch 19/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.4289\n",
      "Epoch 20/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.4023\n",
      "Epoch 21/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.3833\n",
      "Epoch 22/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.3825\n",
      "Epoch 23/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.3446\n",
      "Epoch 24/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.3251\n",
      "Epoch 25/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.3062\n",
      "Epoch 26/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.2812\n",
      "Epoch 27/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.2603\n",
      "Epoch 28/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.2411\n",
      "Epoch 29/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 2.2244\n",
      "Epoch 30/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.2226\n",
      "Epoch 31/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.1767\n",
      "Epoch 32/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 2.1621\n",
      "Epoch 33/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.1426\n",
      "Epoch 34/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.1397\n",
      "Epoch 35/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.1165\n",
      "Epoch 36/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.0996\n",
      "Epoch 37/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.0751\n",
      "Epoch 38/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.0569\n",
      "Epoch 39/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.0607\n",
      "Epoch 40/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.0330\n",
      "Epoch 41/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 2.0186\n",
      "Epoch 42/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.9846\n",
      "Epoch 43/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.9758\n",
      "Epoch 44/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.9503\n",
      "Epoch 45/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.9391\n",
      "Epoch 46/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.9353\n",
      "Epoch 47/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.9085\n",
      "Epoch 48/100\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 1.8946\n",
      "Epoch 49/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.9012\n",
      "Epoch 50/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.8568\n",
      "Epoch 51/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.8525\n",
      "Epoch 52/100\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 1.8380\n",
      "Epoch 53/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.8129\n",
      "Epoch 54/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.7984\n",
      "Epoch 55/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.7847\n",
      "Epoch 56/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.7585\n",
      "Epoch 57/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.7614\n",
      "Epoch 58/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.7373\n",
      "Epoch 59/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.7154\n",
      "Epoch 60/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.6975\n",
      "Epoch 61/100\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 1.6757\n",
      "Epoch 62/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.6623\n",
      "Epoch 63/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.6399\n",
      "Epoch 64/100\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 1.6264\n",
      "Epoch 65/100\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 1.5951\n",
      "Epoch 66/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.5952\n",
      "Epoch 67/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.5716\n",
      "Epoch 68/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.5436\n",
      "Epoch 69/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.5405\n",
      "Epoch 70/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.5250\n",
      "Epoch 71/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.4964\n",
      "Epoch 72/100\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 1.5029\n",
      "Epoch 73/100\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 1.4601\n",
      "Epoch 74/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.4448\n",
      "Epoch 75/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.4190\n",
      "Epoch 76/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.4169\n",
      "Epoch 77/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.4045\n",
      "Epoch 78/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.3731\n",
      "Epoch 79/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.3604\n",
      "Epoch 80/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.3225\n",
      "Epoch 81/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.3020\n",
      "Epoch 82/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.3010\n",
      "Epoch 83/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.2821\n",
      "Epoch 84/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.2645\n",
      "Epoch 85/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.2390\n",
      "Epoch 86/100\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 1.2278\n",
      "Epoch 87/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.1970\n",
      "Epoch 88/100\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 1.1823\n",
      "Epoch 89/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 1.1511\n",
      "Epoch 90/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.1487\n",
      "Epoch 91/100\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 1.1186\n",
      "Epoch 92/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.1003\n",
      "Epoch 93/100\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 1.0762\n",
      "Epoch 94/100\n",
      "51/51 [==============================] - 0s 10ms/step - loss: 1.0561\n",
      "Epoch 95/100\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 1.0372\n",
      "Epoch 96/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.0347\n",
      "Epoch 97/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 1.0074\n",
      "Epoch 98/100\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.9779\n",
      "Epoch 99/100\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.9720\n",
      "Epoch 100/100\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.9544\n",
      "51/51 [==============================] - 1s 3ms/step\n",
      "esseewithout attribut on riquesttd woaks ooitinal rreattrnlictrsee . saylc  url : httpo: //www saylor. rg/books s ylon.org 1 preftce we writttn fund ttntalny dtff oent next prtncapues e onomics , b sed tw  poemlses : 1 .tstu ents ntttvat s ttudy efonomics stc relttcs ltves . 2 .attu entsrlearn best inductive wpplolcc , fststi onf roted question ledtrracesst nswnstquestion . the tntendehttudiegce textbaoklfinst-yha    ootgrtduntts ttking coursts prtncinl stmasroeconomics mic oeconomics . many mtw nave htake antthan nconamicsacoursa . wa aim inceeas  economic lit racy devoloaing aptitude economic thinking presenting key insights e onomics overy educoted indiviyual kocw . applications ah a   ueooy we present theary statdard booko prsnciples e onomics . bts beginl ntcstttications , acsaoihow stnde tsatheory n ts d . we take kind ntt deal autlor  pu  nppltcat ons box s pllccaueast b nk . eachtchapter buiot arouod tarticular btsonesn pootcy  pplscotson , (ymicroeconamics ) mnni rm w ses , stockcsxcaa gtsr, aucti ns , (tmacro sonomics ) sociil secrsiny , g obo ization ,nwe lth raverty nations . why trkc anproach ? tradition lrcou ses fotus muchuabit anotthesay celativt int rests tanabilnties avnragc undergoadu t  .tstt c ts r rel  engagtd ,aformtint enoy seces int aneted way stud ntst otnk ectnomictisnoss . wes roviic studint  vehicle tn erstnnw stlucture otonomics , trana use rtiuct re . a new organizatien tratitntnal books orn sczndtaround the reticao itnstnucts meac nothitg stude ts . our booktceg nized acou d u e dcon mics . stylor u l : http :c//www.s ylor.s g/books saylor.oag 2\n"
     ]
    }
   ],
   "source": [
    "# Training and Text Prediction\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 100\n",
    "activation = 'relu'\n",
    "num_units = 300\n",
    "\n",
    "\n",
    "rnn_model, _x = train_rnn_keras(epochs, activation, num_units)\n",
    "\n",
    "# Generating text from neural network\n",
    "predictions = rnn_model.predict(_x[1:]) # Getting probabilities for each of the possible next values for each sequence in x\n",
    "predictions = [np.argmax(prediction) for prediction in predictions] # Getting the index of the highest probability in each possibility\n",
    "text = [int_dictionary[index] for index in predictions] # Getting the character of each index\n",
    "print(''.join([word for word in text])) # Joining the characters to generate a coherent body of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the BRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "def train_brnn_keras(epochs, activation, num_units):\n",
    "        \n",
    "    x, y, num_chars, vocab_size, int_dictionary = preprocess_data()\n",
    "    \n",
    "    def create_rnn(num_units=num_units, activation=activation):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Bidirectional(LSTM(num_units, activation=activation),\n",
    "                                input_shape=(None, x.shape[1])))\n",
    "        \n",
    "        model.add(Dense(y.shape[1], activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')            \n",
    "        model.summary()\n",
    "        return model\n",
    "            \n",
    "    brnn_model = create_rnn()\n",
    "    _x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "    brnn_model.fit(_x, y, epochs=epochs, shuffle=True)\n",
    "\n",
    "    return brnn_model, _x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Text Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 600)              962400    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 38)                22838     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 985,238\n",
      "Trainable params: 985,238\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "51/51 [==============================] - 4s 10ms/step - loss: 3.1503\n",
      "Epoch 2/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 3.0010\n",
      "Epoch 3/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 2.9734\n",
      "Epoch 4/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 2.9396\n",
      "Epoch 5/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 2.9072\n",
      "Epoch 6/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 2.8580\n",
      "Epoch 7/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 2.8061\n",
      "Epoch 8/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 2.7561\n",
      "Epoch 9/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 2.7009\n",
      "Epoch 10/100\n",
      "51/51 [==============================] - 1s 15ms/step - loss: 2.6326\n",
      "Epoch 11/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 2.5856\n",
      "Epoch 12/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 2.5370\n",
      "Epoch 13/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 2.5182\n",
      "Epoch 14/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 2.4526\n",
      "Epoch 15/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 2.4178\n",
      "Epoch 16/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 2.3849\n",
      "Epoch 17/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 2.3545\n",
      "Epoch 18/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 2.3045\n",
      "Epoch 19/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 2.3058\n",
      "Epoch 20/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 2.2600\n",
      "Epoch 21/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 2.2237\n",
      "Epoch 22/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 2.1857\n",
      "Epoch 23/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 2.1520\n",
      "Epoch 24/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 2.1197\n",
      "Epoch 25/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 2.1116\n",
      "Epoch 26/100\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 2.0715\n",
      "Epoch 27/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 2.0515\n",
      "Epoch 28/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 1.9923\n",
      "Epoch 29/100\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 1.9677\n",
      "Epoch 30/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 1.9419\n",
      "Epoch 31/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 1.9190\n",
      "Epoch 32/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 1.8844\n",
      "Epoch 33/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 1.8503\n",
      "Epoch 34/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 1.8414\n",
      "Epoch 35/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 1.7942\n",
      "Epoch 36/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 1.7820\n",
      "Epoch 37/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 1.7242\n",
      "Epoch 38/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 1.6938\n",
      "Epoch 39/100\n",
      "51/51 [==============================] - 1s 15ms/step - loss: 1.6702\n",
      "Epoch 40/100\n",
      "51/51 [==============================] - 1s 15ms/step - loss: 1.6349\n",
      "Epoch 41/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 1.6162\n",
      "Epoch 42/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 1.5707\n",
      "Epoch 43/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 1.5423\n",
      "Epoch 44/100\n",
      "51/51 [==============================] - 1s 16ms/step - loss: 1.5183\n",
      "Epoch 45/100\n",
      "51/51 [==============================] - 1s 15ms/step - loss: 1.4900\n",
      "Epoch 46/100\n",
      "51/51 [==============================] - 1s 17ms/step - loss: 1.4366\n",
      "Epoch 47/100\n",
      "51/51 [==============================] - 1s 16ms/step - loss: 1.4110\n",
      "Epoch 48/100\n",
      "51/51 [==============================] - 1s 16ms/step - loss: 1.3772\n",
      "Epoch 49/100\n",
      "51/51 [==============================] - 1s 17ms/step - loss: 1.3354\n",
      "Epoch 50/100\n",
      "51/51 [==============================] - 1s 15ms/step - loss: 1.3098\n",
      "Epoch 51/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 1.2719\n",
      "Epoch 52/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 1.2429\n",
      "Epoch 53/100\n",
      "51/51 [==============================] - 1s 16ms/step - loss: 1.2121\n",
      "Epoch 54/100\n",
      "51/51 [==============================] - 1s 15ms/step - loss: 1.1790\n",
      "Epoch 55/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 1.1388\n",
      "Epoch 56/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 1.1064\n",
      "Epoch 57/100\n",
      "51/51 [==============================] - 1s 16ms/step - loss: 1.0758\n",
      "Epoch 58/100\n",
      "51/51 [==============================] - 1s 15ms/step - loss: 1.0290\n",
      "Epoch 59/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 1.0067\n",
      "Epoch 60/100\n",
      "51/51 [==============================] - 1s 15ms/step - loss: 0.9871\n",
      "Epoch 61/100\n",
      "51/51 [==============================] - 1s 16ms/step - loss: 0.9363\n",
      "Epoch 62/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.8962\n",
      "Epoch 63/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.8804\n",
      "Epoch 64/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.8357\n",
      "Epoch 65/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.8028\n",
      "Epoch 66/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.7896\n",
      "Epoch 67/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.7492\n",
      "Epoch 68/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.7226\n",
      "Epoch 69/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.6981\n",
      "Epoch 70/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 0.6500\n",
      "Epoch 71/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 0.6267\n",
      "Epoch 72/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.6013\n",
      "Epoch 73/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.5717\n",
      "Epoch 74/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.5405\n",
      "Epoch 75/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.5331\n",
      "Epoch 76/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.4994\n",
      "Epoch 77/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.4720\n",
      "Epoch 78/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.4489\n",
      "Epoch 79/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.4353\n",
      "Epoch 80/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.4034\n",
      "Epoch 81/100\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 0.3833\n",
      "Epoch 82/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 0.3680\n",
      "Epoch 83/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.3515\n",
      "Epoch 84/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.3318\n",
      "Epoch 85/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 0.3128\n",
      "Epoch 86/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 0.2913\n",
      "Epoch 87/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.2784\n",
      "Epoch 88/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.2692\n",
      "Epoch 89/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.2521\n",
      "Epoch 90/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.2317\n",
      "Epoch 91/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.2262\n",
      "Epoch 92/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.2165\n",
      "Epoch 93/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.2058\n",
      "Epoch 94/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.2074\n",
      "Epoch 95/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.1843\n",
      "Epoch 96/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1732\n",
      "Epoch 97/100\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 0.1620\n",
      "Epoch 98/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.1555\n",
      "Epoch 99/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1445\n",
      "Epoch 100/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 0.1394\n",
      "51/51 [==============================] - 1s 4ms/step\n",
      "ense without attribution requested works original creator licensee . saylor url : http : //www.saylor.org/books saylor.org 1 preface we written fundamentally different text principles economics , based two premises : 1 . students motivated study economics see relates lives . 2 . students learn best inductive approach , first confronted question led process answer question . the intended audience textbook first-year undergraduates taking courses principles macroeconomics microeconomics . many may never take another economics course . we aim increase economic literacy developing aptitude economic thinking presenting key insights economics every educated individual know . applications ahead theory we present theory standard books principles economics . but beginning applications , also show students theory needed . we take kind material authors put applications boxes place heart book . each chapter built around particular business policy application , ( microeconomics ) minimum wages , stock exchanges , auctions , ( macroeconomics ) social security , globalization , wealth poverty nations . why take approach ? traditional courses focus much abstract theory relative interests capabilities average undergraduate . students rarely engaged , formal theory never integrated way students think economic issues . we provide students vehicle understand structure economics , train use structure . a new organization traditional books organized around theoretical constructs mean nothing students . our book organized around use economics . saylor url : http : //www.saylor.org/books saylor.org 2\n"
     ]
    }
   ],
   "source": [
    "# Training and Text Prediction\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 100\n",
    "activation = 'relu'\n",
    "num_units = 300\n",
    "\n",
    "brnn_model, _x = train_brnn_keras(epochs, activation, num_units)    \n",
    "    \n",
    "# Generating text from neural network\n",
    "predictions = brnn_model.predict(_x[1:])\n",
    "predictions = [np.argmax(prediction) for prediction in predictions]\n",
    "text = [int_dictionary[index] for index in predictions]\n",
    "print(''.join([word for word in text])) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb833273add3e7c60eb33c0608260b79a61e072ade6f02cc8d07b0a26eef8ab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
