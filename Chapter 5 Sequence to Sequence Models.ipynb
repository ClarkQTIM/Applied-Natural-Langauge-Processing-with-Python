{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary modules \n",
    "import numpy as np, json\n",
    "import tensorflow\n",
    "from tensorflow.python.keras.models import Input\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We do not have access to the qadataset, so this will not run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Parameters\n",
    "n_units = 300\n",
    "epochs = 3\n",
    "batch_size = 50\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join([word for word in text if ord(word) < 128])\n",
    "\n",
    "def load_data():\n",
    "    dataset = json.load(open('/Users/tawehbeysolow/Downloads/qadataset.json', 'rb'))['data']\n",
    "    questions, answers = [], []\n",
    "\n",
    "    # Filling the question and answer lists\n",
    "    for j in range(0, len(dataset)):\n",
    "        for k in range(0, len(dataset[j])):\n",
    "            for i in range(0, len(dataset[j]['paragraphs'][k]['qas'])):\n",
    "                questions.append(remove_non_ascii(dataset[j]['paragraphs'][k]['qas'][i]['question']))\n",
    "                answers.append(remove_non_ascii(dataset[j]['paragraphs'][k]['qas'][i]['answers'][0]['text']))\n",
    "    print('Questions', questions[0:2])\n",
    "    print('Answers', answers[0:2])\n",
    "                \n",
    "    # Gettings sets of input and output characters\n",
    "    input_chars, output_chars = set(), set()\n",
    "    \n",
    "    for i in range(0, len(questions)):\n",
    "        for char in questions[i]: \n",
    "            if char not in input_chars: input_chars.add(char.lower())\n",
    "    \n",
    "    for i in range(0, len(answers)):\n",
    "        for char in answers[i]:\n",
    "            if char not in output_chars: output_chars.add(char.lower())\n",
    "    \n",
    "    input_chars, output_chars = sorted(list(input_chars)), sorted(list(output_chars))\n",
    "    n_encoder_tokens, n_decoder_tokens = len(input_chars), len(output_chars) # The number of tokens of the encoder and decoder are\n",
    "    # just the number of input and output characters\n",
    "    max_encoder_len = max([len(text) for text in questions]) # The encoder will recieve input of the length of the longest question\n",
    "    max_decoder_len = max([len(text) for text in answers]) # The decoder will recieve input of the length of the longest answer\n",
    "    \n",
    "    input_dictionary = {word: i for i, word in enumerate(input_chars)} # Allows us to transform the letters into numbers\n",
    "    output_dictionary = {word: i for i, word in enumerate(output_chars)} # Allows us to transform the letters into numbers\n",
    "    label_dictionary = {i: word for i, word in enumerate(output_chars)} # Goes the other way, from numbers to letters. Will be used\n",
    "    # to transform a sequence of output numbers to words, which we then concatenate to get human-readable outputs\n",
    "    \n",
    "    x_encoder = np.zeros((len(questions), max_encoder_len, n_encoder_tokens))\n",
    "    x_decoder = np.zeros((len(questions), max_decoder_len, n_decoder_tokens))\n",
    "    y_decoder = np.zeros((len(questions), max_decoder_len, n_decoder_tokens))\n",
    "\n",
    "    for i, (input, output) in enumerate(zip(questions, answers)):\n",
    "        for _character, character in enumerate(input):\n",
    "            x_encoder[i, _character, input_dictionary[character.lower()]] = 1.\n",
    "    \n",
    "        for _character, character in enumerate(output):\n",
    "            x_decoder[i, _character, output_dictionary[character.lower()]] = 1.\n",
    "\n",
    "            if i > 0: y_decoder[i, _character, output_dictionary[character.lower()]] = 1.\n",
    "\n",
    "    data = list([x_encoder, x_decoder, y_decoder])      \n",
    "    variables = list([label_dictionary, n_decoder_tokens, n_encoder_tokens])                             \n",
    "    return data, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_objects = load_data()\n",
    "\n",
    "# Getting the x_encoder (letters in the questions), x_decoder (letters in the answers), \n",
    "# y_decoder (letters), label_dictionary, and n_decoder_tokens\n",
    "\n",
    "x_encoder, x_decoder, y_decoder = input_data_objects[0][0], input_data_objects[0][1], input_data_objects[0][2]\n",
    "label_dictionary, n_decoder_tokens = input_data_objects[1][0], input_data_objects[1][1]\n",
    "n_encoder_tokens = input_data_objects[1][2]\n",
    "print('x_encoder', x_encoder[0])\n",
    "print('x_decoder', x_decoder[0])\n",
    "print('y_decoder', y_decoder[0])\n",
    "print('label_dictionary', label_dictionary)\n",
    "print('n_decoder_tokens', n_decoder_tokens)\n",
    "print('n_encoder_tokens', n_encoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder/Decoder and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encoder_decoder(n_encoder_tokens, n_decoder_tokens):\n",
    "    \n",
    "    # Encoder. This transforms a sequences of letters (question) into vectors\n",
    "    encoder_input = Input(shape=(None, n_encoder_tokens))    \n",
    "    encoder = LSTM(n_units, return_state=True)\n",
    "    encoder_output, hidden_state, cell_state = encoder(encoder_input)\n",
    "    encoder_states = [hidden_state, cell_state] # We are taking the hidden state and output of each cell of the encoder LSTM\n",
    "    \n",
    "    # Decoder. This takes the vectors from above and transforms them into vectors corresponding to a possible answer\n",
    "    decoder_input = Input(shape=(None, n_decoder_tokens))\n",
    "    decoder = LSTM(n_units, return_state=True, return_sequences=True)\n",
    "    decoder_output, _, _ = decoder(decoder_input, initial_state=encoder_states)\n",
    "    \n",
    "    # Final Softmax Layer. This takes the answer vectors and outputs actual letters corresponding to the vectors\n",
    "    decoder = Dense(n_decoder_tokens, activation='softmax')(decoder_output)\n",
    "    model = Model([encoder_input, decoder_input], decoder)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def train_encoder_decoder(x_encoder, x_decoder, y_decoder, label_dictionary, n_decoder_tokens, n_encoder_tokens): \n",
    "\n",
    "    # Fitting the model\n",
    "    seq2seq_model = encoder_decoder(n_encoder_tokens, n_decoder_tokens)\n",
    "    seq2seq_model.fit([x_encoder, x_decoder], y_decoder, batch_size=batch_size, epochs=epochs, shuffle=True)\n",
    "    \n",
    "    #Comparing model predictions and actual labels\n",
    "    for start, end in zip(range(0, 10, 1), range(1, 11, 1)):\n",
    "        y_predict = seq2seq_model.predict([x_encoder[start:end], x_decoder[start:end]])\n",
    "        input_sequences, output_sequences = [], []\n",
    "        for i in range(0, len(y_predict[0])): \n",
    "            output_sequences.append(np.argmax(y_predict[0][i]))\n",
    "            input_sequences.append(np.argmax(x_decoder[start][i]))\n",
    "        \n",
    "        output_sequences = ''.join([label_dictionary[key] for key in output_sequences])\n",
    "        input_sequences = ''.join([label_dictionary[key] for key in input_sequences])\n",
    "        print('Model Prediction: ' + output_sequences); print('Actual Output: ' + input_sequences)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoder_decoder(x_encoder, x_decoder, y_decoder, label_dictionary, n_decoder_tokens, n_encoder_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb833273add3e7c60eb33c0608260b79a61e072ade6f02cc8d07b0a26eef8ab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
