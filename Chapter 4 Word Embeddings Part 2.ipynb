{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\chris\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Tensorflow Version 2.9.1\n"
     ]
    }
   ],
   "source": [
    "# Standard Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "import random \n",
    "\n",
    "# Processing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Neural Nets\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tensorflow as tf2\n",
    "from tensorflow import keras\n",
    "print('Tensorflow Version',tf.__version__)\n",
    "\n",
    "# NLP\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "from collections import namedtuple # For Doc2Vec\n",
    "\n",
    "# PDF manipulation\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "# Other\n",
    "import collections\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful source: https://blog.paperspace.com/pre-trained-word-embeddings-natural-language-processing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and Needed Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for our model\n",
    "\n",
    "learning_rate = 1e-4; n_input = 4; \n",
    "n_hidden = 500; epochs = 300\n",
    "offset=10; n_units = n_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2)/float(np.linalg.norm(v1)*np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text we wish to add to our model\n",
    "\n",
    "sample_text = '''Living in different places has been the greatest experience \n",
    "that I have had in my life. It has allowed me to understand people from \n",
    "different walks of life, as well as to question some of my own biases I have had \n",
    "with respect to people who did not grow up as I did. If possible, everyone should \n",
    "take an opportunity to travel somewhere separate from where they grew up'''.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for our model\n",
    "\n",
    "learning_rate = 1e-4; n_input = 4; \n",
    "n_hidden = 500; epochs = 300\n",
    "offset=10; n_units = n_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained embedding\n",
    "\n",
    "def load_embedding(embedding_path='Data/glove.6B.50D.txt'):\n",
    "    vocabulary, embedding, embedding_dictionary = [], [], {}\n",
    "    for line in open(embedding_path, 'r', encoding=\"utf8\").readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocabulary.append(row[0]), embedding.append(row[1:])\n",
    "        embedding_vector = [float(i) for i in row[1:]]\n",
    "        embedding_dictionary[row[0]] = embedding_vector\n",
    "    vocabulary_length, embedding_dim = len(vocabulary), len(embedding[0])\n",
    "    return vocabulary, np.asarray(embedding, dtype=float), vocabulary_length, embedding_dim, embedding_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the pre-trained embedding\n",
    "\n",
    "def visualize_embedding_example():\n",
    "    \n",
    "    vocabulary, embedding, vocabulary_length, embedding_dim, embedding_dictionary = load_embedding()\n",
    "    # We run the above load_embedding function to give us information about the embedding we are adding to\n",
    "\n",
    "    #Showing example of pretrained word embedding vectors\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_embedding = pca.fit_transform(embedding)\n",
    "    plt.scatter(pca_embedding[0:50, 0], pca_embedding[0:50, 1])\n",
    "    for i, word in enumerate(vocabulary[0:50]):\n",
    "        plt.annotate(word, xy=(pca_embedding[i, 0], pca_embedding[i, 1]))\n",
    "        \n",
    "    #Comparing cosine similarity \n",
    "    for k in range(100, 105):\n",
    "        text = str('Cosine Similarity Between %s and %s: %s')%(vocabulary[k],\n",
    "                                                            vocabulary[k-1], \n",
    "                                                cosine_similarity(embedding[k], \n",
    "                                                                  embedding[k-1]))\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Between so and u.s.: 0.5606769548631282\n",
      "Cosine Similarity Between them and so: 0.8815159254335487\n",
      "Cosine Similarity Between what and them: 0.8077565084355356\n",
      "Cosine Similarity Between him and what: 0.7972281857691554\n",
      "Cosine Similarity Between united and him: 0.5374600664967558\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_embedding_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, embedding, vocabulary_length, embedding_dim, embedding_dictionary = load_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding [[ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      "  -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      "  -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      "  -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      "  -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "   4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "   1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      "  -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      "  -1.1514e-01 -7.8581e-01]\n",
      " [ 1.3441e-02  2.3682e-01 -1.6899e-01  4.0951e-01  6.3812e-01  4.7709e-01\n",
      "  -4.2852e-01 -5.5641e-01 -3.6400e-01 -2.3938e-01  1.3001e-01 -6.3734e-02\n",
      "  -3.9575e-01 -4.8162e-01  2.3291e-01  9.0201e-02 -1.3324e-01  7.8639e-02\n",
      "  -4.1634e-01 -1.5428e-01  1.0068e-01  4.8891e-01  3.1226e-01 -1.2520e-01\n",
      "  -3.7512e-02 -1.5179e+00  1.2612e-01 -2.4420e-02 -4.2961e-02 -2.8351e-01\n",
      "   3.5416e+00 -1.1956e-01 -1.4533e-02 -1.4990e-01  2.1864e-01 -3.3412e-01\n",
      "  -1.3872e-01  3.1806e-01  7.0358e-01  4.4858e-01 -8.0262e-02  6.3003e-01\n",
      "   3.2111e-01 -4.6765e-01  2.2786e-01  3.6034e-01 -3.7818e-01 -5.6657e-01\n",
      "   4.4691e-02  3.0392e-01]\n",
      " [ 1.5164e-01  3.0177e-01 -1.6763e-01  1.7684e-01  3.1719e-01  3.3973e-01\n",
      "  -4.3478e-01 -3.1086e-01 -4.4999e-01 -2.9486e-01  1.6608e-01  1.1963e-01\n",
      "  -4.1328e-01 -4.2353e-01  5.9868e-01  2.8825e-01 -1.1547e-01 -4.1848e-02\n",
      "  -6.7989e-01 -2.5063e-01  1.8472e-01  8.6876e-02  4.6582e-01  1.5035e-02\n",
      "   4.3474e-02 -1.4671e+00 -3.0384e-01 -2.3441e-02  3.0589e-01 -2.1785e-01\n",
      "   3.7460e+00  4.2284e-03 -1.8436e-01 -4.6209e-01  9.8329e-02 -1.1907e-01\n",
      "   2.3919e-01  1.1610e-01  4.1705e-01  5.6763e-02 -6.3681e-05  6.8987e-02\n",
      "   8.7939e-02 -1.0285e-01 -1.3931e-01  2.2314e-01 -8.0803e-02 -3.5652e-01\n",
      "   1.6413e-02  1.0216e-01]\n",
      " [ 7.0853e-01  5.7088e-01 -4.7160e-01  1.8048e-01  5.4449e-01  7.2603e-01\n",
      "   1.8157e-01 -5.2393e-01  1.0381e-01 -1.7566e-01  7.8852e-02 -3.6216e-01\n",
      "  -1.1829e-01 -8.3336e-01  1.1917e-01 -1.6605e-01  6.1555e-02 -1.2719e-02\n",
      "  -5.6623e-01  1.3616e-02  2.2851e-01 -1.4396e-01 -6.7549e-02 -3.8157e-01\n",
      "  -2.3698e-01 -1.7037e+00 -8.6692e-01 -2.6704e-01 -2.5890e-01  1.7670e-01\n",
      "   3.8676e+00 -1.6130e-01 -1.3273e-01 -6.8881e-01  1.8444e-01  5.2464e-03\n",
      "  -3.3874e-01 -7.8956e-02  2.4185e-01  3.6576e-01 -3.4727e-01  2.8483e-01\n",
      "   7.5693e-02 -6.2178e-02 -3.8988e-01  2.2902e-01 -2.1617e-01 -2.2562e-01\n",
      "  -9.3918e-02 -8.0375e-01]\n",
      " [ 6.8047e-01 -3.9263e-02  3.0186e-01 -1.7792e-01  4.2962e-01  3.2246e-02\n",
      "  -4.1376e-01  1.3228e-01 -2.9847e-01 -8.5253e-02  1.7118e-01  2.2419e-01\n",
      "  -1.0046e-01 -4.3653e-01  3.3418e-01  6.7846e-01  5.7204e-02 -3.4448e-01\n",
      "  -4.2785e-01 -4.3275e-01  5.5963e-01  1.0032e-01  1.8677e-01 -2.6854e-01\n",
      "   3.7334e-02 -2.0932e+00  2.2171e-01 -3.9868e-01  2.0912e-01 -5.5725e-01\n",
      "   3.8826e+00  4.7466e-01 -9.5658e-01 -3.7788e-01  2.0869e-01 -3.2752e-01\n",
      "   1.2751e-01  8.8359e-02  1.6351e-01 -2.1634e-01 -9.4375e-02  1.8324e-02\n",
      "   2.1048e-01 -3.0880e-02 -1.9722e-01  8.2279e-02 -9.4340e-02 -7.3297e-02\n",
      "  -6.4699e-02 -2.6044e-01]]\n",
      "embedding dictionary [('the', [0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566, 0.27843, -0.14767, -0.55677, 0.14658, -0.0095095, 0.011658, 0.10204, -0.12792, -0.8443, -0.12181, -0.016801, -0.33279, -0.1552, -0.23131, -0.19181, -1.8823, -0.76746, 0.099051, -0.42125, -0.19526, 4.0071, -0.18594, -0.52287, -0.31681, 0.00059213, 0.0074449, 0.17778, -0.15897, 0.012041, -0.054223, -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581]), (',', [0.013441, 0.23682, -0.16899, 0.40951, 0.63812, 0.47709, -0.42852, -0.55641, -0.364, -0.23938, 0.13001, -0.063734, -0.39575, -0.48162, 0.23291, 0.090201, -0.13324, 0.078639, -0.41634, -0.15428, 0.10068, 0.48891, 0.31226, -0.1252, -0.037512, -1.5179, 0.12612, -0.02442, -0.042961, -0.28351, 3.5416, -0.11956, -0.014533, -0.1499, 0.21864, -0.33412, -0.13872, 0.31806, 0.70358, 0.44858, -0.080262, 0.63003, 0.32111, -0.46765, 0.22786, 0.36034, -0.37818, -0.56657, 0.044691, 0.30392]), ('.', [0.15164, 0.30177, -0.16763, 0.17684, 0.31719, 0.33973, -0.43478, -0.31086, -0.44999, -0.29486, 0.16608, 0.11963, -0.41328, -0.42353, 0.59868, 0.28825, -0.11547, -0.041848, -0.67989, -0.25063, 0.18472, 0.086876, 0.46582, 0.015035, 0.043474, -1.4671, -0.30384, -0.023441, 0.30589, -0.21785, 3.746, 0.0042284, -0.18436, -0.46209, 0.098329, -0.11907, 0.23919, 0.1161, 0.41705, 0.056763, -6.3681e-05, 0.068987, 0.087939, -0.10285, -0.13931, 0.22314, -0.080803, -0.35652, 0.016413, 0.10216]), ('of', [0.70853, 0.57088, -0.4716, 0.18048, 0.54449, 0.72603, 0.18157, -0.52393, 0.10381, -0.17566, 0.078852, -0.36216, -0.11829, -0.83336, 0.11917, -0.16605, 0.061555, -0.012719, -0.56623, 0.013616, 0.22851, -0.14396, -0.067549, -0.38157, -0.23698, -1.7037, -0.86692, -0.26704, -0.2589, 0.1767, 3.8676, -0.1613, -0.13273, -0.68881, 0.18444, 0.0052464, -0.33874, -0.078956, 0.24185, 0.36576, -0.34727, 0.28483, 0.075693, -0.062178, -0.38988, 0.22902, -0.21617, -0.22562, -0.093918, -0.80375]), ('to', [0.68047, -0.039263, 0.30186, -0.17792, 0.42962, 0.032246, -0.41376, 0.13228, -0.29847, -0.085253, 0.17118, 0.22419, -0.10046, -0.43653, 0.33418, 0.67846, 0.057204, -0.34448, -0.42785, -0.43275, 0.55963, 0.10032, 0.18677, -0.26854, 0.037334, -2.0932, 0.22171, -0.39868, 0.20912, -0.55725, 3.8826, 0.47466, -0.95658, -0.37788, 0.20869, -0.32752, 0.12751, 0.088359, 0.16351, -0.21634, -0.094375, 0.018324, 0.21048, -0.03088, -0.19722, 0.082279, -0.09434, -0.073297, -0.064699, -0.26044])]\n",
      "[-0.033537   0.47537   -0.68746   -0.72661    0.84028    0.64304\n",
      " -0.75975    0.63242   -0.54176    0.11632   -0.20254    0.63321\n",
      " -1.2677    -0.17674    0.35284   -0.55096   -0.65025   -0.3405\n",
      " -0.31658   -0.077908  -0.11085    0.97299   -0.016844  -0.73752\n",
      "  0.47852   -2.7069    -0.42417   -0.053489   0.018467  -0.11892\n",
      "  3.3082     0.17864   -0.50702   -0.22894    0.24178    0.5698\n",
      "  0.097113   0.95422    0.0076093 -0.54154    0.09828    0.41533\n",
      " -1.116      0.0050954 -0.14975   -0.45133   -0.081188  -0.62173\n",
      " -0.022628  -0.4383   ]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "print('embedding', embedding[0:5])\n",
    "print('embedding dictionary', take(5, embedding_dictionary.items()))\n",
    "print(embedding[26, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data_example(vocabulary, embedding,\n",
    "                          vocabulary_length, embedding_dim, embedding_dictionary,\n",
    "                          sample_text=sample_text, learning_rate=learning_rate, \n",
    "                          n_input=n_input, n_hidden=n_hidden, epochs=epochs,\n",
    "                          offset=offset, n_units=n_units):\n",
    "                          \n",
    "    _sample_text = np.array(sample_text.split())\n",
    "    _sample_text = _sample_text.reshape([-1, ])\n",
    "    print('Sample text', _sample_text)\n",
    "    _sample_embedding_array = []\n",
    "\n",
    "    def sample_text_dictionary(data=_sample_text):\n",
    "        count, dictionary = collections.Counter(data).most_common(), {} # Creates list of word/count pairs;\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = len(dictionary) # Len(dictionary) increases each iteration\n",
    "            reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        dictionary_list = sorted(dictionary.items(), key = lambda x : x[1])\n",
    "        return dictionary, reverse_dictionary, dictionary_list\n",
    "        \n",
    "    sample_dictionary, sample_reverse_dictionary, sample_dictionary_list = sample_text_dictionary()\n",
    "    \n",
    "    for i in range(len(sample_dictionary)):\n",
    "        word = sample_dictionary_list[i][0]\n",
    "        if word in vocabulary:\n",
    "            _sample_embedding_array.append(embedding_dictionary[word])\n",
    "        else:\n",
    "            _sample_embedding_array.append(np.random.uniform(low=-0.2, high=0.2, size=embedding_dim))\n",
    "            print('This word is not in the pretrained embedding dictionary:', word)\n",
    "     \n",
    "    sample_embedding_array = np.asarray(_sample_embedding_array)     \n",
    "    decision_tree = spatial.KDTree(sample_embedding_array, leafsize=100)\n",
    "    print('Sample dictionary', sample_dictionary)\n",
    "\n",
    "    # Initializing placeholders and other variables\n",
    "\n",
    "    X = tf.placeholder(tf.int32, shape=(None, None, n_input)) # We will fill this with x_train, a sequence of n_input words, later on\n",
    "    # print('X', X, '\\n')\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, embedding_dim)) # We will fill this with y_train later on\n",
    "    # print('Y', Y, '\\n')\n",
    "    weights = {'output': tf.Variable(tf.random_normal([n_hidden, embedding_dim]))} # Random initial weights\n",
    "    # print('weights', weights, '\\n')\n",
    "    biases = {'output': tf.Variable(tf.random_normal([embedding_dim]))} # Random initial biases\n",
    "    # print('baises', biases, '\\n')\n",
    "\n",
    "    # Setting up the computational graph to update the pretrained embedding\n",
    "    \n",
    "    _weights = tf.Variable(tf.constant(0.0, shape=[vocabulary_length, embedding_dim]), trainable=True) # Weights which we will assign\n",
    "    _embedding = tf.placeholder(tf.float32, [vocabulary_length, embedding_dim]) # We want to actually train our embedding\n",
    "    embedding_initializer = _weights.assign(_embedding) # The weights are coming from the embedding\n",
    "    embedding_characters = tf.nn.embedding_lookup(_weights, X) # This will give us the weights (which are the embeddings, from the line above)\n",
    "    # of our X values (words) in each epoch from the embedding fed to the 'embedding_initializer'\n",
    "        \n",
    "    input_series = tf.reshape(embedding_characters, [-1, n_input])\n",
    "    input_series = tf.split(input_series, n_input, 1) # Now we have the weights from the embedding of choice\n",
    "    # which will be the pretrained, in this case, for each word\n",
    "\n",
    "    # Creating our model\n",
    "\n",
    "    lstm_cell =   tf2.keras.layers.LSTMCell(units=n_units, activation=tf.nn.relu) # LSTM cell\n",
    "    outputs, states = tf.compat.v1.nn.static_rnn(lstm_cell, input_series, dtype=tf.float32) # Running the input through the LSTM cell\n",
    "    # to get the outputs and states    \n",
    "    output_layer = tf.add(tf.matmul(outputs[-1], weights['output']), biases['output']) # Getting y_hat using the weights and biases\n",
    "    # calculated just after 'Initializing placeholders...'\n",
    "    error = tf.reduce_mean(tf.nn.l2_loss(output_layer - Y)) # Error\n",
    "    adam_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(error) # Optimization with Adam to optimize the\n",
    "    # weights, biases, AND the pretrained weights (embedding)\n",
    "\n",
    "    # Training \n",
    "    \n",
    "    with tf.Session() as sess: # Running the training\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            if offset+n_input >= len(sample_reverse_dictionary): # Allowing us to train on random chunks of the training data (words)\n",
    "                offset = random.randint(0, n_input+1)\n",
    "                print('Hitting random offset', '\\n')\n",
    "           \n",
    "            sess.run(embedding_initializer, feed_dict={_embedding: embedding}) # Setting the embedding as our pretrained one\n",
    "            \n",
    "            # Creating input and output training data\n",
    "            x_train = [[sample_dictionary[str(_sample_text[i])]] for i in range(offset, offset+n_input)] # Creating random training set\n",
    "            # of size n_input\n",
    "            x_train = np.reshape(np.array(x_train), [-1, 1, n_input]) # Reshaping the training set\n",
    "            # print('X_train', x_train, '\\n')\n",
    "            word = _sample_text[offset+n_input]\n",
    "            word_index = sample_dictionary[word]\n",
    "            y_train = sample_dictionary[_sample_text[offset+n_input]] # Getting the correct next word's index in the sample_dictionary\n",
    "            y_train = embedding[y_train, :] # Getting the pretrained embedding at the index given in the previous line\n",
    "            y_train = np.reshape(y_train, [1, -1])\n",
    "            # print('y_train', y_train, '\\n')\n",
    "    \n",
    "            _, _error, _prediction = sess.run([adam_optimizer, error, output_layer], \n",
    "                                     feed_dict = {X: x_train, Y: y_train})\n",
    "            \n",
    "            if epoch%10 == 0 and epoch > 0: # Checking how our NN is doing\n",
    "                input_sequence = [str(_sample_text[i]) for i in range(offset, offset+n_input)] # Getting a sequence of words\n",
    "                target_word = str(_sample_text[offset+n_input]) # Getting the correct word\n",
    "                distance, _index = decision_tree.query(_prediction[0], 1) # Getting the predicted word's index using the sample_text\n",
    "                # embedding\n",
    "                predicted_word = sample_reverse_dictionary[_index] # Getting the predicted word\n",
    "                  \n",
    "                print('Input Sequence: %s \\nActual Label: %s \\nPredicted Label: %s'%\n",
    "                      (input_sequence, target_word, predicted_word))\n",
    "                print('Epoch: %s \\nError: %s \\n'%(epoch, _error))\n",
    "                offset += (n_input+1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text ['Living' 'in' 'different' 'places' 'has' 'been' 'the' 'greatest'\n",
      " 'experience' 'that' 'I' 'have' 'had' 'in' 'my' 'life.' 'It' 'has'\n",
      " 'allowed' 'me' 'to' 'understand' 'people' 'from' 'different' 'walks' 'of'\n",
      " 'life,' 'as' 'well' 'as' 'to' 'question' 'some' 'of' 'my' 'own' 'biases'\n",
      " 'I' 'have' 'had' 'with' 'respect' 'to' 'people' 'who' 'did' 'not' 'grow'\n",
      " 'up' 'as' 'I' 'did.' 'If' 'possible,' 'everyone' 'should' 'take' 'an'\n",
      " 'opportunity' 'to' 'travel' 'somewhere' 'separate' 'from' 'where' 'they'\n",
      " 'grew' 'up']\n",
      "This word is not in the pretrained embedding dictionary: I\n",
      "This word is not in the pretrained embedding dictionary: Living\n",
      "This word is not in the pretrained embedding dictionary: life.\n",
      "This word is not in the pretrained embedding dictionary: It\n",
      "This word is not in the pretrained embedding dictionary: life,\n",
      "This word is not in the pretrained embedding dictionary: did.\n",
      "This word is not in the pretrained embedding dictionary: If\n",
      "This word is not in the pretrained embedding dictionary: possible,\n",
      "Sample dictionary {'to': 0, 'I': 1, 'as': 2, 'in': 3, 'different': 4, 'has': 5, 'have': 6, 'had': 7, 'my': 8, 'people': 9, 'from': 10, 'of': 11, 'up': 12, 'Living': 13, 'places': 14, 'been': 15, 'the': 16, 'greatest': 17, 'experience': 18, 'that': 19, 'life.': 20, 'It': 21, 'allowed': 22, 'me': 23, 'understand': 24, 'walks': 25, 'life,': 26, 'well': 27, 'question': 28, 'some': 29, 'own': 30, 'biases': 31, 'with': 32, 'respect': 33, 'who': 34, 'did': 35, 'not': 36, 'grow': 37, 'did.': 38, 'If': 39, 'possible,': 40, 'everyone': 41, 'should': 42, 'take': 43, 'an': 44, 'opportunity': 45, 'travel': 46, 'somewhere': 47, 'separate': 48, 'where': 49, 'they': 50, 'grew': 51}\n",
      "WARNING:tensorflow:From C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_11848\\2873461676.py:60: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "Input Sequence: ['I', 'have', 'had', 'in'] \n",
      "Actual Label: my \n",
      "Predicted Label: possible,\n",
      "Epoch: 10 \n",
      "Error: 2192.5544 \n",
      "\n",
      "Input Sequence: ['life.', 'It', 'has', 'allowed'] \n",
      "Actual Label: me \n",
      "Predicted Label: possible,\n",
      "Epoch: 20 \n",
      "Error: 2063.0154 \n",
      "\n",
      "Input Sequence: ['to', 'understand', 'people', 'from'] \n",
      "Actual Label: different \n",
      "Predicted Label: possible,\n",
      "Epoch: 30 \n",
      "Error: 1728.8153 \n",
      "\n",
      "Input Sequence: ['walks', 'of', 'life,', 'as'] \n",
      "Actual Label: well \n",
      "Predicted Label: possible,\n",
      "Epoch: 40 \n",
      "Error: 1964.0303 \n",
      "\n",
      "Input Sequence: ['as', 'to', 'question', 'some'] \n",
      "Actual Label: of \n",
      "Predicted Label: possible,\n",
      "Epoch: 50 \n",
      "Error: 1638.4019 \n",
      "\n",
      "Input Sequence: ['my', 'own', 'biases', 'I'] \n",
      "Actual Label: have \n",
      "Predicted Label: possible,\n",
      "Epoch: 60 \n",
      "Error: 1140.5424 \n",
      "\n",
      "Input Sequence: ['had', 'with', 'respect', 'to'] \n",
      "Actual Label: people \n",
      "Predicted Label: possible,\n",
      "Epoch: 70 \n",
      "Error: 1023.30133 \n",
      "\n",
      "Input Sequence: ['who', 'did', 'not', 'grow'] \n",
      "Actual Label: up \n",
      "Predicted Label: possible,\n",
      "Epoch: 80 \n",
      "Error: 1011.1753 \n",
      "\n",
      "Hitting random offset \n",
      "\n",
      "Input Sequence: ['in', 'different', 'places', 'has'] \n",
      "Actual Label: been \n",
      "Predicted Label: Living\n",
      "Epoch: 90 \n",
      "Error: 772.6638 \n",
      "\n",
      "Input Sequence: ['the', 'greatest', 'experience', 'that'] \n",
      "Actual Label: I \n",
      "Predicted Label: possible,\n",
      "Epoch: 100 \n",
      "Error: 602.51965 \n",
      "\n",
      "Input Sequence: ['have', 'had', 'in', 'my'] \n",
      "Actual Label: life. \n",
      "Predicted Label: Living\n",
      "Epoch: 110 \n",
      "Error: 586.3723 \n",
      "\n",
      "Input Sequence: ['It', 'has', 'allowed', 'me'] \n",
      "Actual Label: to \n",
      "Predicted Label: as\n",
      "Epoch: 120 \n",
      "Error: 357.09033 \n",
      "\n",
      "Input Sequence: ['understand', 'people', 'from', 'different'] \n",
      "Actual Label: walks \n",
      "Predicted Label: the\n",
      "Epoch: 130 \n",
      "Error: 328.07352 \n",
      "\n",
      "Input Sequence: ['of', 'life,', 'as', 'well'] \n",
      "Actual Label: as \n",
      "Predicted Label: in\n",
      "Epoch: 140 \n",
      "Error: 245.20844 \n",
      "\n",
      "Input Sequence: ['to', 'question', 'some', 'of'] \n",
      "Actual Label: my \n",
      "Predicted Label: as\n",
      "Epoch: 150 \n",
      "Error: 506.9086 \n",
      "\n",
      "Input Sequence: ['own', 'biases', 'I', 'have'] \n",
      "Actual Label: had \n",
      "Predicted Label: as\n",
      "Epoch: 160 \n",
      "Error: 249.02722 \n",
      "\n",
      "Input Sequence: ['with', 'respect', 'to', 'people'] \n",
      "Actual Label: who \n",
      "Predicted Label: as\n",
      "Epoch: 170 \n",
      "Error: 202.50777 \n",
      "\n",
      "Input Sequence: ['did', 'not', 'grow', 'up'] \n",
      "Actual Label: as \n",
      "Predicted Label: as\n",
      "Epoch: 180 \n",
      "Error: 132.35739 \n",
      "\n",
      "Hitting random offset \n",
      "\n",
      "Input Sequence: ['has', 'been', 'the', 'greatest'] \n",
      "Actual Label: experience \n",
      "Predicted Label: as\n",
      "Epoch: 190 \n",
      "Error: 198.9106 \n",
      "\n",
      "Input Sequence: ['that', 'I', 'have', 'had'] \n",
      "Actual Label: in \n",
      "Predicted Label: as\n",
      "Epoch: 200 \n",
      "Error: 198.85194 \n",
      "\n",
      "Input Sequence: ['my', 'life.', 'It', 'has'] \n",
      "Actual Label: allowed \n",
      "Predicted Label: as\n",
      "Epoch: 210 \n",
      "Error: 278.05634 \n",
      "\n",
      "Input Sequence: ['me', 'to', 'understand', 'people'] \n",
      "Actual Label: from \n",
      "Predicted Label: in\n",
      "Epoch: 220 \n",
      "Error: 133.7828 \n",
      "\n",
      "Input Sequence: ['different', 'walks', 'of', 'life,'] \n",
      "Actual Label: as \n",
      "Predicted Label: in\n",
      "Epoch: 230 \n",
      "Error: 113.26398 \n",
      "\n",
      "Input Sequence: ['well', 'as', 'to', 'question'] \n",
      "Actual Label: some \n",
      "Predicted Label: as\n",
      "Epoch: 240 \n",
      "Error: 168.47092 \n",
      "\n",
      "Input Sequence: ['of', 'my', 'own', 'biases'] \n",
      "Actual Label: I \n",
      "Predicted Label: in\n",
      "Epoch: 250 \n",
      "Error: 113.262344 \n",
      "\n",
      "Input Sequence: ['have', 'had', 'with', 'respect'] \n",
      "Actual Label: to \n",
      "Predicted Label: as\n",
      "Epoch: 260 \n",
      "Error: 117.9471 \n",
      "\n",
      "Input Sequence: ['people', 'who', 'did', 'not'] \n",
      "Actual Label: grow \n",
      "Predicted Label: the\n",
      "Epoch: 270 \n",
      "Error: 117.057335 \n",
      "\n",
      "Hitting random offset \n",
      "\n",
      "Input Sequence: ['different', 'places', 'has', 'been'] \n",
      "Actual Label: the \n",
      "Predicted Label: the\n",
      "Epoch: 280 \n",
      "Error: 415.24765 \n",
      "\n",
      "Input Sequence: ['greatest', 'experience', 'that', 'I'] \n",
      "Actual Label: have \n",
      "Predicted Label: has\n",
      "Epoch: 290 \n",
      "Error: 152.94865 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data_example(vocabulary, embedding, vocabulary_length, embedding_dim, embedding_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: It seems as if the way thsi algorithm is set up has us grabbing the index of the word from the sample_dictionary and then just using that index and getting the embedding from the pretrained embedding at the index and using that as our goal. But, there doesn't seem to be any relationship between the sample_dictionary keys/values and the pretrained embedding dictionary. Why are we doing this?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "fb833273add3e7c60eb33c0608260b79a61e072ade6f02cc8d07b0a26eef8ab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
