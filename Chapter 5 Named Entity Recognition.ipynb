{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary modules\n",
    "import numpy as np, math, string, pandas as pan, time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Dense, Embedding, TimeDistributed\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another good tool: https://towardsdatascience.com/named-entity-recognition-ner-using-keras-bidirectional-lstm-28cd3f301f54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Parameters\n",
    "np.random.seed(2018); learning_rate = 0.01; momentum = 0.9\n",
    "activation = 'selu'; out_act = 'softmax'; opt = 'adam'\n",
    "n_units = 1000; batch_size = 32; punctuation = set(string.punctuation)\n",
    "input_shape = 75; epochs = 1; validation_split = 0.10; output_dim = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def load_data():\n",
    "    text_data = open('Data/train.txt', 'r').readlines()\n",
    "    text_data = [text_data[k].replace('\\t', ' ').split() for k in range(0, len(text_data))]\n",
    "    print('Text example after being split up is a list of words, then parts-of-speech, then the tag', text_data[0:3], '\\n')\n",
    "    index = range(0, len(text_data), 3)\n",
    "    \n",
    "    #Transforming data to matrix format for neural network\n",
    "    input_data =  list()\n",
    "    for i in range(1, len(index)-1):\n",
    "        rows = text_data[index[i-1]:index[i]]\n",
    "        sentence_no = np.array([i for i in np.repeat(i, len(rows[0]))], dtype=str)\n",
    "        rows.append(np.array(sentence_no))\n",
    "        rows = np.array(rows).T\n",
    "        input_data.append(rows)\n",
    "    \n",
    "    input_data = pan.DataFrame(np.concatenate([input_data[j] for j in range(0,len(input_data))]), \n",
    "                           columns=['word', 'pos', 'tag', 'sent_no'])\n",
    "    print('input_data as a dataframe/matrix', input_data.head(), '\\n')\n",
    "    \n",
    "    labels, vocabulary = list(set(input_data['tag'].values)), list(set(input_data['word'].values))\n",
    "    vocabulary.append('endpad'); vocab_size = len(vocabulary); label_size = len(labels)\n",
    "    \n",
    "    aggregate_function = lambda input: [(word, pos, label) for word, pos, label in zip(input['word'].values.tolist(),\n",
    "                                                      input['pos'].values.tolist(),\n",
    "                                                       input['tag'].values.tolist())]\n",
    "                           \n",
    "    grouped_input_data= input_data.groupby('sent_no').apply(aggregate_function)\n",
    "    print('grouped_input_data', grouped_input_data[0:5], '\\n')\n",
    "    sentences = [sentence for sentence in grouped_input_data]\n",
    "    word_dictionary = {word: i for i, word in enumerate(vocabulary)}\n",
    "    label_dictionary = {label: i for i, label in enumerate(labels)}\n",
    "    output_dictionary = {i: labels for i, labels in enumerate(labels)}\n",
    "    x = [[word_dictionary[word[0]] for word in sent] for sent in sentences]    \n",
    "    x = pad_sequences(maxlen=input_shape, sequences=x, padding='post', value=0)\n",
    "    y = [[label_dictionary[word[2]] for word in sent] for sent in sentences]  \n",
    "    y = pad_sequences(maxlen=input_shape, sequences=y, padding='post', value=0)\n",
    "    y = [np_utils.to_categorical(label, num_classes=label_size) for label in y]            \n",
    "    return x, y, output_dictionary, vocab_size, label_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs of the above functions for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text example after being split up is a list of words, then parts-of-speech, then the tag [['played', 'on', 'Monday', '(', 'home', 'team', 'in', 'CAPS', ')', ':'], ['VBD', 'IN', 'NNP', '(', 'NN', 'NN', 'IN', 'NNP', ')', ':'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']] \n",
      "\n",
      "input_data as a dataframe/matrix      word  pos tag sent_no\n",
      "0  played  VBD   O       1\n",
      "1      on   IN   O       1\n",
      "2  Monday  NNP   O       1\n",
      "3       (    (   O       1\n",
      "4    home   NN   O       1 \n",
      "\n",
      "grouped_input_data sent_no\n",
      "1        [(played, VBD, O), (on, IN, O), (Monday, NNP, ...\n",
      "10       [(SAN, NNP, B-ORG), (FRANCISCO, NNP, I-ORG), (...\n",
      "100      [(VfB, NNP, B-ORG), (Stuttgart, NNP, I-ORG), (...\n",
      "1000     [(Results, NNS, O), (of, IN, O), (Major, NNP, ...\n",
      "10000    [(Hartlepool, NNP, B-ORG), (2, CD, O), (Fulham...\n",
      "dtype: object \n",
      "\n",
      "output_dict {0: 'sO', 1: 'B-LOC', 2: 'I-ORG', 3: 'I-LOC', 4: 'B-MISC', 5: 'I-MISC', 6: 'B-PER', 7: 'I-PER', 8: 'O', 9: 'B-ORG'} \n",
      "\n",
      "vocab size 24340 \n",
      "\n",
      "label size 10 \n",
      "\n",
      "x [ 6397 23757 10339 18289 12921  3665 18014  9058 11522 17970     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0] \n",
      "\n",
      "x.shape (13998, 75) \n",
      "\n",
      "y.shape 75 \n",
      "\n",
      "y [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, y, output_dictionary, vocab_size, label_size = load_data()\n",
    "\n",
    "print('output_dict', output_dictionary, '\\n')\n",
    "print('vocab size', vocab_size, '\\n')\n",
    "print('label size', label_size, '\\n')\n",
    "print('x', x[0], '\\n')\n",
    "print('x.shape', x.shape, '\\n')\n",
    "print('y.shape', len(y[0]), '\\n')\n",
    "print('y', y[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, each instance of x is a *padded* sentence of integers corresponding to the words. Each y is a $75 \\times 10$ matrix, where each of the 75 lists of length 10 is one-hot encoding of the tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def train_brnn_keras():\n",
    "\n",
    "    train_end = int(math.floor(len(x)*.80))\n",
    "    train_x, train_y = x[0:train_end] , np.array(y[0:train_end])\n",
    "    test_x, test_y = x[train_end:], np.array(y[train_end:])\n",
    "    \n",
    "    def create_brnn():\n",
    "        model = Sequential()\n",
    "        # The Embedding is to transform the input into the correct shape (batch_size, sent_length, output_dim)\n",
    "        model.add(Embedding(input_dim=vocab_size+1, output_dim=output_dim,\n",
    "                            input_length=input_shape, mask_zero=True))\n",
    "        model.add(Bidirectional(LSTM(units=n_units, activation=activation,\n",
    "                                     return_sequences=True)))\n",
    "        # The TimeDistributed is because we are doing a Many-to-Many input and output scheme. So, this will provide\n",
    "        # an output (a one-hot encoding of the tag) for each integer (representing a word in the sentence)\n",
    "        model.add(TimeDistributed(Dense(label_size, activation=out_act)))\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    lstm_model = create_brnn()\n",
    "    lstm_model.fit(train_x, train_y, epochs=epochs, shuffle=True, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    for start, end in zip(range(0, 10, 1), range(1, 11, 1)):\n",
    "        y_predict = lstm_model.predict(test_x[start:end])\n",
    "        input_sequences, output_sequences =  [], []\n",
    "        for i in range(0, len(y_predict[0])): \n",
    "            output_sequences.append(np.argmax(y_predict[0][i]))\n",
    "            input_sequences.append(np.argmax(test_y[start][0]))\n",
    "        \n",
    "        print('Test Accuracy: ' + str(lstm_model.evaluate(test_x[start:end], test_y[start:end])))\n",
    "        output_sequences = ' '.join([output_dictionary[key] for key in output_sequences]).split()\n",
    "        input_sequences = ' '.join([output_dictionary[key] for key in input_sequences]).split()\n",
    "        output_input_comparison = pan.DataFrame([output_sequences, input_sequences]).T\n",
    "        print(output_input_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 75, 20)            486820    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 75, 2000)         8168000   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 75, 10)           20010     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,674,830\n",
      "Trainable params: 8,674,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "350/350 [==============================] - 1859s 5s/step - loss: 0.0995 - accuracy: 0.8641\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.4950e-04 - accuracy: 1.0000\n",
      "Test Accuracy: [0.00014949627802707255, 1.0]\n",
      "    0  1\n",
      "0   O  O\n",
      "1   O  O\n",
      "2   O  O\n",
      "3   O  O\n",
      "4   O  O\n",
      ".. .. ..\n",
      "70  O  O\n",
      "71  O  O\n",
      "72  O  O\n",
      "73  O  O\n",
      "74  O  O\n",
      "\n",
      "[75 rows x 2 columns]\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0694 - accuracy: 0.9333\n",
      "Test Accuracy: [0.06938689947128296, 0.9333333373069763]\n",
      "        0      1\n",
      "0   B-ORG  B-PER\n",
      "1   I-PER  B-PER\n",
      "2       O  B-PER\n",
      "3       O  B-PER\n",
      "4       O  B-PER\n",
      "..    ...    ...\n",
      "70      O  B-PER\n",
      "71      O  B-PER\n",
      "72      O  B-PER\n",
      "73      O  B-PER\n",
      "74      O  B-PER\n",
      "\n",
      "[75 rows x 2 columns]\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Test Accuracy: [0.0018325811251997948, 1.0]\n",
      "    0  1\n",
      "0   O  O\n",
      "1   O  O\n",
      "2   O  O\n",
      "3   O  O\n",
      "4   O  O\n",
      ".. .. ..\n",
      "70  O  O\n",
      "71  O  O\n",
      "72  O  O\n",
      "73  O  O\n",
      "74  O  O\n",
      "\n",
      "[75 rows x 2 columns]\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0352 - accuracy: 0.6667\n",
      "Test Accuracy: [0.035208337008953094, 0.6666666865348816]\n",
      "        0  1\n",
      "0       O  O\n",
      "1       O  O\n",
      "2   B-ORG  O\n",
      "3   I-ORG  O\n",
      "4       O  O\n",
      "..    ... ..\n",
      "70      O  O\n",
      "71      O  O\n",
      "72      O  O\n",
      "73      O  O\n",
      "74      O  O\n",
      "\n",
      "[75 rows x 2 columns]\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Test Accuracy: [0.0023842621594667435, 1.0]\n",
      "        0      1\n",
      "0   B-LOC  B-LOC\n",
      "1       O  B-LOC\n",
      "2       O  B-LOC\n",
      "3       O  B-LOC\n",
      "4       O  B-LOC\n",
      "..    ...    ...\n",
      "70      O  B-LOC\n",
      "71      O  B-LOC\n",
      "72      O  B-LOC\n",
      "73      O  B-LOC\n",
      "74      O  B-LOC\n",
      "\n",
      "[75 rows x 2 columns]\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0194 - accuracy: 0.8000\n",
      "Test Accuracy: [0.019411342218518257, 0.800000011920929]\n",
      "        0  1\n",
      "0       O  O\n",
      "1       O  O\n",
      "2       O  O\n",
      "3   B-LOC  O\n",
      "4       O  O\n",
      "..    ... ..\n",
      "70      O  O\n",
      "71      O  O\n",
      "72      O  O\n",
      "73      O  O\n",
      "74      O  O\n",
      "\n",
      "[75 rows x 2 columns]\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Test Accuracy: [0.001949360710568726, 1.0]\n",
      "    0  1\n",
      "0   O  O\n",
      "1   O  O\n",
      "2   O  O\n",
      "3   O  O\n",
      "4   O  O\n",
      ".. .. ..\n",
      "70  O  O\n",
      "71  O  O\n",
      "72  O  O\n",
      "73  O  O\n",
      "74  O  O\n",
      "\n",
      "[75 rows x 2 columns]\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 7.2516e-04 - accuracy: 1.0000\n",
      "Test Accuracy: [0.0007251582574099302, 1.0]\n",
      "    0  1\n",
      "0   O  O\n",
      "1   O  O\n",
      "2   O  O\n",
      "3   O  O\n",
      "4   O  O\n",
      ".. .. ..\n",
      "70  O  O\n",
      "71  O  O\n",
      "72  O  O\n",
      "73  O  O\n",
      "74  O  O\n",
      "\n",
      "[75 rows x 2 columns]\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Test Accuracy: [0.010261639021337032, 1.0]\n",
      "        0      1\n",
      "0   B-ORG  B-ORG\n",
      "1   I-ORG  B-ORG\n",
      "2       O  B-ORG\n",
      "3       O  B-ORG\n",
      "4       O  B-ORG\n",
      "..    ...    ...\n",
      "70      O  B-ORG\n",
      "71      O  B-ORG\n",
      "72      O  B-ORG\n",
      "73      O  B-ORG\n",
      "74      O  B-ORG\n",
      "\n",
      "[75 rows x 2 columns]\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Test Accuracy: [0.00411079591140151, 1.0]\n",
      "        0      1\n",
      "0   B-ORG  B-ORG\n",
      "1       O  B-ORG\n",
      "2       O  B-ORG\n",
      "3       O  B-ORG\n",
      "4       O  B-ORG\n",
      "..    ...    ...\n",
      "70      O  B-ORG\n",
      "71      O  B-ORG\n",
      "72      O  B-ORG\n",
      "73      O  B-ORG\n",
      "74      O  B-ORG\n",
      "\n",
      "[75 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "lstm_model = train_brnn_keras()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "fb833273add3e7c60eb33c0608260b79a61e072ade6f02cc8d07b0a26eef8ab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
