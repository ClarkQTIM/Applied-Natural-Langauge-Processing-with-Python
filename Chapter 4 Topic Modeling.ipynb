{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471495d1",
   "metadata": {},
   "source": [
    "# Setting up our Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea90b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re, gensim\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81290d3b",
   "metadata": {},
   "source": [
    "# Loading in the Data and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29ed05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to remove characters that will cause unicode errors\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join([word for word in text if ord(word) < 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739371a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    # os.listdir() method in python is used to get the \n",
    "    # list of all files and directories in the specified directory. \n",
    "    \n",
    "    negative_review_strings = os.listdir('Movie Reviews/review_data/tokens/neg')\n",
    "    positive_review_strings = os.listdir('Movie Reviews/review_data/tokens/pos')\n",
    "    negative_reviews, positive_reviews = [], []\n",
    "    \n",
    "    for positive_review in positive_review_strings:\n",
    "        with open('Movie Reviews/review_data/tokens/pos/'+str(positive_review), 'r') as positive_file:\n",
    "            positive_reviews.append(remove_non_ascii(positive_file.read()))\n",
    "    # Note, since we have the \"open()\" as 'r', we need to do file.read() to access it\n",
    "    \n",
    "    for negative_review in negative_review_strings:\n",
    "        with open('Movie Reviews/review_data/tokens/neg/'+str(negative_review), 'r') as negative_file:\n",
    "            negative_reviews.append(remove_non_ascii(negative_file.read()))\n",
    "    \n",
    "    negative_labels, positive_labels = np.repeat(0, len(negative_reviews)), np.repeat(1, len(positive_reviews))\n",
    "    # This just makes a bunch of 0s and 1s\n",
    "    \n",
    "    labels = np.concatenate([negative_labels, positive_labels])\n",
    "    # Getting our full list of 0s and 1s\n",
    "    reviews = np.concatenate([negative_reviews, positive_reviews])\n",
    "    # Gettings all our reviews in order of negative then positive\n",
    "    \n",
    "    # From here, we could sample the rows if we wanted, or we could leave it the full dataset\n",
    "#     rows = np.random.random_integers(0, len(reviews)-1, len(reviews)-1) \n",
    "#     data = pd.DataFrame(np.array([labels[rows], reviews[rows]]).T, columns=['Label', 'Text'])\n",
    "\n",
    "    data = pd.DataFrame(np.array([labels, reviews]).T, columns=['Label', 'Text'])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "658b4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5985991d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tristar / 1 : 30 / 1997 / r ( language , viole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>arlington road 1/4 . directed by mark pellingt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>the brady bunch movie is less a motion picture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>janeane garofalo in a romantic comedy -- it wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i'm going to keep this plot summary brief , so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text\n",
       "0     0  tristar / 1 : 30 / 1997 / r ( language , viole...\n",
       "1     0  arlington road 1/4 . directed by mark pellingt...\n",
       "2     0  the brady bunch movie is less a motion picture...\n",
       "3     0  janeane garofalo in a romantic comedy -- it wa...\n",
       "4     0  i'm going to keep this plot summary brief , so..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "017e3049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       tristar / 1 : 30 / 1997 / r ( language , viole...\n",
       "1       arlington road 1/4 . directed by mark pellingt...\n",
       "2       the brady bunch movie is less a motion picture...\n",
       "3       janeane garofalo in a romantic comedy -- it wa...\n",
       "4       i'm going to keep this plot summary brief , so...\n",
       "                              ...                        \n",
       "1395    one of the last entries in the long-running ca...\n",
       "1396    hype ? sheesh , like no other . this side of t...\n",
       "1397    for those of us who weren't yet born when the ...\n",
       "1398    what starts out as a monotonous talking-head m...\n",
       "1399    jackie brown ( miramax - 1997 ) starring pam g...\n",
       "Name: Text, Length: 1400, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_data['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf4df97",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0343d5",
   "metadata": {},
   "source": [
    "## Mathematical Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4af7b8",
   "metadata": {},
   "source": [
    "*Latent Dirichlet Allocation* is a form of unsupervised learning meant for topic modeling. We begin by defining several notations for our LDA: \n",
    "\n",
    "1. A word is a basic unit of data in a vocabulary, $V$. \n",
    "2. A document is a vector of words of length $N$, $\\mathbf{w}=(w_1,….,w_n)$. Note, all documents don’t have to have the same length, but it makes the math easier and doesn’t change anything substantial. \n",
    "3. A corpus is a collection of $M$ documents, which will be denoted $D=(\\mathbf{w}_1,…,\\mathbf{w}_M)$\n",
    "\n",
    "Further, we assume that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. In other words, we have a Dirichlet distribution describing the distribution of documents among topics and another Dirichlet distribution describing the distribution of topics among words. Within each document, we pick topics through a multinomial distribution and within each topic, we pick words through a multinomial distribution, as well.\n",
    "\n",
    "Consider that we believe we have $K$ topics in our corpus. Then, because we are modeling the distribution of topics among the documents as a Dirichlet distribution, we have a $K$-dimensional vector per document, $\\mathbf{\\theta}_m \\in \\Delta^{K-1}$, $\\mathbf{\\theta}_{m,k} \\in [0,1]$ representing the share of all topics in document m drawn from the Dirichlet distribution with parameters $\\mathbf{\\alpha}$. Then, we have a $V$-dimensional vector per topic $\\mathbf{\\phi}_k \\in \\Delta^{V-1}$, $\\mathbf{\\theta}_{k,V} \\in [0,1]$, which is the vector of word-probabilities in topic $k$ chosen from a Dirichlet distribution with parameters $\\mathbf{\\beta}$. Then, we write the topic associated with word $n$ in document $m$, or the topic associated with $w_{m,n}$, as $z_{m,n}\\in(1,…,K)$. We can also write our list of topics, with length $K$, as $\\mathbf{z}$. With this, we can write out our document generation procedure.\n",
    "1. Draw $M$ parameter vectors $\\mathbf{\\theta}_m$ from $Dir(\\mathbf{\\alpha})$\n",
    "2. Draw $K$ parameter vectors per document $\\mathbf{\\phi}_k$ from $Dir(\\mathbf{\\beta})$\n",
    "3. Each word $w_{m,n}$ in document $d$ is generated by:\n",
    "4. Draw $z_{m,n}$ from $Multi(\\mathbf{\\theta}_d)$\n",
    "5. Draw $w_{m,n}$ from $Multi(\\mathbf{\\phi}_{z_{m,n}})$\n",
    "\n",
    "Then, our model, can be written as:\n",
    "\n",
    "$$P(\\mathbf{W}, \\mathbf{Z}, \\mathbf{\\Theta}, \\mathbf{\\Phi} | \\mathbf{\\alpha}, \\mathbf{\\beta})=\\prod_{k=1}^{K}p(\\mathbf{\\phi}_k|\\mathbf{\\beta})\\prod_{m=1}^{M}p(\\mathbf{\\theta}_m|\\mathbf{\\alpha})\\prod_{n=1}^{N}p(z_{m,n}|\\mathbf{\\theta}_d)p(w_{m,n}|\\mathbf{\\phi}_{z_{m,n}})$$\n",
    "\n",
    "In the above, $\\mathbf{W}$ is the collection of all words in all documents, $\\mathbf{Z}$ is the collection of all topics in all documents, $\\mathbf{\\Theta}$ is the collection of all topic-document distributions, and $\\mathbf{\\Phi}$ is the collection of all word-topic distributions. This is thus our probability model for an entire corpus. \n",
    "\n",
    "We then need to train our model, often used, for example, in scki-kit learn and Gensim, *Online Variation Bayes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "823c17a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2018)\n",
    "n_topics = 10\n",
    "stop_words = stopwords.words('english')\n",
    "n_frequent_words = 1500\n",
    "n_components = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a88d09d",
   "metadata": {},
   "source": [
    "## LDA with BoW and TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4585f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e24d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_topic_model(feat_extractor):\n",
    "    \n",
    "    def create_topic_model(model, n_topics=10, max_iter=5, min_df=10, \n",
    "                           max_df=300, stop_words='english', token_pattern=r'\\w+'):\n",
    "        \n",
    "        print(model + ' topic model:')\n",
    "        data = load_data()['Text']\n",
    "        if feat_extractor == 'bow':\n",
    "            feature_extractor = CountVectorizer(min_df=min_df, max_df=max_df, \n",
    "                                                stop_words=stop_words, token_pattern=token_pattern)\n",
    "        elif feat_extractor == 'tfidf':\n",
    "            feature_extractor = TfidfVectorizer(min_df=min_df, max_df=max_df, \n",
    "                                                stop_words=stop_words, token_pattern=token_pattern)\n",
    "            \n",
    "        processed_data = feature_extractor.fit_transform(data) # Transforming the data into the form we want\n",
    "        lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online', \n",
    "                                              learning_offset=50., max_iter=max_iter, verbose=0) # Defining the model      \n",
    "        lda_model.fit(processed_data) # Fitting the model\n",
    "        features = feature_extractor.get_feature_names_out() # Getting the features\n",
    "        print_topics(model=lda_model, feature_names=features, n_top_words=n_topics)\n",
    "\n",
    "    create_topic_model(model=feat_extractor)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e19e9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow topic model:\n",
      "Topic #0: uk 2000 patch 1 visit produced photographed copyright certificate scale\n",
      "Topic #1: farrelly senseless wayans marlon joshua senses awake bowling kingpin angel\n",
      "Topic #2: original family michael david guy american d effects woman high\n",
      "Topic #3: sandler wedding crystal adam burns singer billy francis buddy julia\n",
      "Topic #4: neeson wars liam lucas phantom jedi zeta menace haunting jones\n",
      "Topic #5: sam macdonald taste amusement revenge weaver ensues entirety sandler juice\n",
      "Topic #6: ruth mcgregor house residents angela simon paul imagery bleak warren\n",
      "Topic #7: van damme en n claude z met er die knock\n",
      "Topic #8: apes carpenter ape sly mars professor snake jane ghosts jungle\n",
      "Topic #9: epps ribisi squad ellis danes blonde giovanni omar silver claire\n"
     ]
    }
   ],
   "source": [
    "sklearn_topic_model('bow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5ca71c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf topic model:\n",
      "Topic #0: mars carpenter franklin ghosts gibson killer sexual paul evil daughter\n",
      "Topic #1: wild west smith tom patch family kline peter deep chris\n",
      "Topic #2: grant l hell helen van jessica mario reeves jones question\n",
      "Topic #3: family michael original david 10 effects woman american guy high\n",
      "Topic #4: redman bvoice bloomington godzilla blonde sex michael gorilla francis contacted\n",
      "Topic #5: sandler harrelson woody lynch virus russell item michael series drive\n",
      "Topic #6: 10 garofalo ribisi todd danes squad epps seagal dialogue watching\n",
      "Topic #7: redman 2000 kim girls jackie spice macdonald wrestling daniel uk\n",
      "Topic #8: perry natasha outside species patrick reviewed beast alec smart attributes\n",
      "Topic #9: mary angel stupid irene kevin breakfast cool bizarre jr richard\n"
     ]
    }
   ],
   "source": [
    "sklearn_topic_model('tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921e8f25",
   "metadata": {},
   "source": [
    "## LDA with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9738ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_topic_model():\n",
    "    \n",
    "    def remove_stop_words(text): \n",
    "        word_tokens = word_tokenize(text.lower()) # Lower-case the words\n",
    "        word_tokens = [word for word in word_tokens if word not in stop_words and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', word)] # Removing stop words\n",
    "        return word_tokens\n",
    "\n",
    "    data = load_data()['Text']\n",
    "    cleaned_data = [remove_stop_words(data[i]) for i in range(0, len(data))]  # Implementing above function\n",
    "    print('Cleaned Data Shape:', len(cleaned_data))\n",
    "    dictionary = gensim.corpora.Dictionary(cleaned_data) # Generating a dictionary for the cleaned data\n",
    "    print('Dictionary shape:', len(dictionary))\n",
    "    dictionary.filter_extremes(no_below=500, no_above=1000) # Removing words appearing below or above a threshold\n",
    "    corpus = [dictionary.doc2bow(text) for text in cleaned_data] # Running each document through the dictionary\n",
    "    # to filter our the too-low and too-frequent, then transforming each document into a BoW\n",
    "    print('Corpus shape:', len(corpus))\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=n_topics, id2word=dictionary)   \n",
    "    print('Gensim LDA implemenation: ')\n",
    "    for _id in range(n_topics):\n",
    "        header = str('Topic #%s: '%(_id))  \n",
    "        tail = str(lda_model.print_topic(_id, 10))\n",
    "        print(header + tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b77a790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data Shape: 1400\n",
      "Dictionary shape: 38960\n",
      "Corpus shape: 1400\n",
      "Gensim LDA implemenation: \n",
      "Topic #0: 0.099*\"movie\" + 0.061*\"even\" + 0.057*\"one\" + 0.052*\"like\" + 0.048*\"film\" + 0.036*\"good\" + 0.030*\"plot\" + 0.030*\"movies\" + 0.030*\"get\" + 0.029*\"also\"\n",
      "Topic #1: 0.051*\"life\" + 0.047*\"film\" + 0.046*\"would\" + 0.045*\"movies\" + 0.044*\"movie\" + 0.043*\"like\" + 0.042*\"one\" + 0.038*\"much\" + 0.036*\"time\" + 0.034*\"two\"\n",
      "Topic #2: 0.084*\"one\" + 0.078*\"movie\" + 0.054*\"like\" + 0.048*\"film\" + 0.046*\"plot\" + 0.042*\"would\" + 0.039*\"two\" + 0.034*\"time\" + 0.031*\"really\" + 0.030*\"could\"\n",
      "Topic #3: 0.109*\"film\" + 0.073*\"one\" + 0.048*\"like\" + 0.043*\"story\" + 0.039*\"first\" + 0.038*\"much\" + 0.037*\"would\" + 0.032*\"characters\" + 0.032*\"movie\" + 0.031*\"good\"\n",
      "Topic #4: 0.139*\"film\" + 0.056*\"one\" + 0.052*\"even\" + 0.041*\"time\" + 0.038*\"like\" + 0.031*\"people\" + 0.031*\"character\" + 0.030*\"movie\" + 0.029*\"would\" + 0.027*\"much\"\n",
      "Topic #5: 0.176*\"film\" + 0.044*\"one\" + 0.044*\"movie\" + 0.039*\"time\" + 0.038*\"like\" + 0.035*\"much\" + 0.032*\"man\" + 0.031*\"good\" + 0.029*\"also\" + 0.029*\"story\"\n",
      "Topic #6: 0.133*\"film\" + 0.046*\"films\" + 0.043*\"one\" + 0.040*\"movie\" + 0.040*\"also\" + 0.037*\"really\" + 0.037*\"like\" + 0.034*\"first\" + 0.033*\"time\" + 0.029*\"see\"\n",
      "Topic #7: 0.088*\"like\" + 0.071*\"film\" + 0.056*\"movie\" + 0.041*\"know\" + 0.040*\"one\" + 0.038*\"even\" + 0.033*\"get\" + 0.033*\"well\" + 0.033*\"movies\" + 0.030*\"good\"\n",
      "Topic #8: 0.116*\"movie\" + 0.106*\"film\" + 0.074*\"one\" + 0.039*\"like\" + 0.031*\"good\" + 0.027*\"time\" + 0.026*\"even\" + 0.025*\"little\" + 0.024*\"well\" + 0.024*\"two\"\n",
      "Topic #9: 0.095*\"one\" + 0.089*\"movie\" + 0.054*\"like\" + 0.054*\"film\" + 0.031*\"characters\" + 0.031*\"get\" + 0.031*\"way\" + 0.029*\"much\" + 0.028*\"scenes\" + 0.028*\"even\"\n"
     ]
    }
   ],
   "source": [
    "gensim_topic_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d07f71",
   "metadata": {},
   "source": [
    "# Non-Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed66681",
   "metadata": {},
   "source": [
    "# Mathematical Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc70bdf",
   "metadata": {},
   "source": [
    "In *Non-Negative Matrix Factorization*, we seek to factor a non-negative matrix $X$ into two matrices $W$ and $H$, also non-negative. The size of the matrices can be chosen and represents something about the problem at hand. For this script, given that we are doing topic modeling, our matrix $X$ will be a matrix of features of a corpus, for example, Bag-of-Words or $TFIDF$, and, after specifying the number of topics $K$, we will have $W$, an $m \\times K$ matrix, where the columns represent the topics, and $H$, a $K \\times n$ matrix, where each row is a word embedding allowing us to reconstruct the documents. \n",
    "\n",
    "Then, $X=WH$. There are several ways to find these matrices, but the following function from ski-kit learn will minimize the the following loss function:\n",
    "\n",
    "$$ \\mathcal{L}(W,H)=d_{loss}(W,H)+\\left(\\alpha_W \\rho ||W||_1+\\frac{\\alpha_W (1-\\rho)}{2}||W||^2 _{Fro}\\right)n_{feat} +\n",
    "\\left(\\alpha_H \\rho ||H||_1 +\\frac{\\alpha_H (1-\\rho)}{2}||H||^2 _{Fro}\\right)n_{samples}$$\n",
    "\n",
    "In the above, the $d_{loss}(W,H)$ can be:\n",
    "\n",
    "$$ d_{Fro}(X,WH)=\\frac{1}{2}||X-WH||^2 _{Fro}=\\frac{1}{2} \\sum_{i,j} (X_{i,j}-WH_{i,j})^2$$\n",
    "\n",
    "$$ d_{KLD}(X,WH) = \\sum_{i,j} \\left( X_{i,j}log (\\frac{X_{i,j}}{Y_{i,j}}) -X_{i,j} +Y_{i,j} \\right)$$\n",
    "\n",
    "$$ d_{IS}(X,WH) = \\sum_{i,j} \\left( \\frac{X_{i,j}}{Y_{i,j}}-log (\\frac{X_{i,j}}{Y_{i,j}}) -1 \\right)$$\n",
    "\n",
    "Finally, $||A||_1$ is the elementwise $L1$ norm, or $||A||_1 = \\sum_{i,j} |A_{i,j}|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8074eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_topic_model(feat_extractor):\n",
    "\n",
    "    def create_topic_model(model, n_topics=10, max_iter=5, min_df=10, \n",
    "                           max_df=300, stop_words='english', token_pattern=r'\\w+'):\n",
    "        print(model + ' NMF topic model: ')\n",
    "        data = load_data()['Text']\n",
    "        if feat_extractor == 'bow':\n",
    "            feature_extractor = CountVectorizer(min_df=min_df, max_df=max_df, \n",
    "                                                stop_words=stop_words, token_pattern=token_pattern)\n",
    "        elif feat_extractor == 'tfidf':\n",
    "            feature_extractor = TfidfVectorizer(min_df=min_df, max_df=max_df, \n",
    "                                                stop_words=stop_words, token_pattern=token_pattern)\n",
    "\n",
    "        processed_data = feature_extractor.fit_transform(data)\n",
    "        nmf_model = NMF(n_components=n_topics, max_iter=max_iter)      \n",
    "        nmf_model.fit(processed_data)\n",
    "        features = feature_extractor.get_feature_names_out()\n",
    "        print_topics(model=nmf_model, feature_names=features, n_top_words=n_topics)\n",
    "        return nmf_model, processed_data, feature_extractor\n",
    "           \n",
    "    create_topic_model(model=feat_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5e21618a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow NMF topic model: \n",
      "Topic #0: woman wife michael d david job high guy play sense\n",
      "Topic #1: jackie tarantino brown chan master drunken hong arts martial fu\n",
      "Topic #2: scream 2 horror williamson sidney craven killer 3 sequel kevin\n",
      "Topic #3: effects special space planet wars ship alien earth science computer\n",
      "Topic #4: disney original family toy animated voice children woody joe kids\n",
      "Topic #5: black van war white american soldiers men lee battle sweet\n",
      "Topic #6: 10 7 8 joblo 5 didn guy 4 critique reviews\n",
      "Topic #7: chicken run park gibson lord mel nick rocky idea voice\n",
      "Topic #8: wild jay max sam kevin smith sex campbell van west\n",
      "Topic #9: alien patrick species html shannon original ca mun rated r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "C:\\Users\\chris\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 5 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nmf_topic_model('bow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c626ef6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf NMF topic model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "C:\\Users\\chris\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 5 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: woman american wife sex city david black night job half\n",
      "Topic #1: alien effects ship space aliens special planet mars virus sci\n",
      "Topic #2: scream horror williamson 2 craven sidney killer stab kevin campbell\n",
      "Topic #3: jackie chan tarantino brown tucker mr martial hong arts grier\n",
      "Topic #4: 10 joblo 7 8 5 critique 4 visit 9 didn\n",
      "Topic #5: redman bvoice bloomington michael mailto indiana 20redman contacted archive reviews_by\n",
      "Topic #6: disney animated animation toy family voice joe children bug kids\n",
      "Topic #7: van en damme n z met soldier er knock wild\n",
      "Topic #8: sandler wedding adam harry singer armageddon julia robbie willis gilmore\n",
      "Topic #9: mail reviews ott ejohnsonott nuvo subscribe e johnson carpenter bloom\n"
     ]
    }
   ],
   "source": [
    "nmf_topic_model('tfidf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
